# AIWorkshop

**AIWorkshop** is a **Blazor** project that demonstrates how to use **Microsoft.Agents.AI.OpenAI** to build AI-driven features focused on **prompt engineering, evaluation, and interaction**.

The application integrates AI agents to analyze prompts, evaluate them against the **TCREI pattern** (based on Google's Prompt Engineering best practices), generate improvements, visualize scores, and answer user questions.

---

## ğŸ“¦ NuGet Package

This project relies on the following preview package:

```xml
<PackageReference Include="Microsoft.Agents.AI.OpenAI" Version="1.0.0-preview.260128.1" />
```

> âš ï¸ **Note**: This is a preview package. APIs and behaviors may change in future versions.

---

## ğŸ§  Key Features

### ğŸ  Home Page â€“ Player Positions

* Displays the **positions of players** (domain-specific entities defined by the project).
* Acts as the main dashboard and entry point to the application.
* Built with Blazor components for a responsive and interactive UI.

---

### ğŸ§ª Prompt Evaluation (TCREI Analyzer)

This screen is dedicated to **prompt analysis and improvement** using AI Agents.

#### What it does:

* Accepts a user-defined prompt
* Evaluates it against the **TCREI pattern**:

  * **T**ask clarity
  * **C**ontext
  * **R**ole
  * **E**xamples
  * **I**nstructions

#### AI-Powered Outputs:

* âœ… **Score per concept** (each TCREI dimension receives a note)
* ğŸ“Š **Graph visualization** showing how each concept scored
* ğŸ§® **Total score** (overall prompt quality)
* âœ¨ **Improved prompt** automatically generated by the AI Agent

All evaluations, scoring, and improvements are generated using **Agent AI** powered by OpenAI models.

---

### â“ Questions Screen

* Allows users to ask **free-form questions**.
* Uses AI Agents to provide contextual and accurate answers.
* Can leverage prior prompt context or domain rules depending on configuration.

---

## ğŸ¤– AI Architecture

* Uses **Microsoft.Agents.AI.OpenAI** to:

  * Create AI agents
  * Send prompts for evaluation
  * Receive structured feedback and improved prompts
* Agents are responsible for:

  * Prompt scoring logic
  * TCREI pattern evaluation
  * Prompt rewriting and optimization

---

## ğŸ› ï¸ Technology Stack

* **Blazor** (Server or WebAssembly)
* **.NET**
* **Microsoft.Agents.AI.OpenAI** (Preview)
* **OpenAI-compatible models**
* **Charting library** (for prompt score visualization)

---

## ğŸš€ Getting Started

1. Clone the repository
2. Restore NuGet packages
3. Configure your OpenAI / Azure OpenAI credentials
4. Run the Blazor application
5. Navigate through:

   * Home (Player Positions)
   * Prompt Evaluation (TCREI)
   * Questions

---

## ğŸ” Configuration

Make sure your AI credentials are configured correctly (for example in `appsettings.json`):

```json
{
  "OpenAI": {
    "ApiKey": "",
    "Provider": "OpenAI"
  }
}
```

---

## ğŸ“ˆ Future Improvements

* Persist prompt evaluations
* Compare multiple prompts side-by-side
* Export prompt reports
* Custom TCREI weighting
* More advanced agent collaboration

---

## ğŸ“„ License

This project is provided as-is for learning and experimentation purposes.

---

## ğŸ‘¤ Author

**Mateus Cambraia**

---

Feel free to contribute, suggest improvements, or adapt this project to your own AI-driven Blazor applications ğŸš€
